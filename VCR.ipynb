{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Video Colorization with Reference",
      "provenance": [],
      "collapsed_sections": [],
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/VCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><font color=\"black\" size=\"+4\">RefColor v.1.0</font></b>\n",
        "\n",
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=mlart.RefColor&left_color=black&right_color=orange)\n",
        "\n",
        "<b><font color=\"black\" size=\"+2\">Based on:</font></b>\n",
        "\n",
        "**GitHub repository**: [Deep-Exemplar-based-Video-Colorization](https://github.com/zhangmozhe/Deep-Exemplar-based-Video-Colorization), [BLIP](https://github.com/salesforce/BLIP)\n",
        "\n",
        "Creator: **[zhangmozhe](https://github.com/zhangmozhe), [salesforce](https://github.com/salesforce)**\n",
        "\n",
        "<b><font color=\"black\" size=\"+2\">Colab created by:</font></b>\n",
        "\n",
        "GitHub: [@tg-bomze](https://github.com/tg-bomze),\n",
        "Telegram: [@MLArt](https://t.me/MLArt),\n",
        "Twitter: [@tg_bomze](https://twitter.com/tg_bomze).\n",
        "\n",
        "```\n",
        "(ENG) To get started, click on the button (where the red arrow indicates). After clicking, wait until the execution is complete.\n",
        "```\n",
        "```\n",
        "(RUS) Чтобы начать, поочередно нажимайте на кнопки (куда указывают красные стрелки), дожидаясь завершения выполнения каждого блока.\n",
        "```"
      ],
      "metadata": {
        "id": "vPBLbFSYSRAh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Dajmr52Kez",
        "cellView": "form"
      },
      "source": [
        "#@title <b><font color=\"red\" size=\"+3\">←</font><font color=\"black\" size=\"+3\"> Install all necessary libraries</font></b>\n",
        "#@markdown <b><font color=\"black\" size=\"+1\"> (RUS) Установить все необходимые библиотеки</font></b>\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import IPython\n",
        "from IPython.display import clear_output\n",
        "from google.colab import output\n",
        "output.disable_custom_widget_manager()\n",
        "%cd /content\n",
        "!git clone https://github.com/zhangmozhe/Deep-Exemplar-based-Video-Colorization.git DEVC\n",
        "%cd /content/DEVC\n",
        "!pip install --upgrade scenedetect[opencv]\n",
        "!pip install -r requirements.txt\n",
        "!wget https://facevc.blob.core.windows.net/zhanbo/old_photo/colorization_checkpoint.zip\n",
        "!unzip colorization_checkpoint.zip\n",
        "\n",
        "def color_restore(orig, color):\n",
        "    orig_yuv = cv2.cvtColor(orig, cv2.COLOR_BGR2YUV)\n",
        "    color_yuv = cv2.cvtColor(color, cv2.COLOR_BGR2YUV)\n",
        "    orig_yuv[:, :, 1:3] = color_yuv[:, :, 1:3]\n",
        "    return cv2.cvtColor(orig_yuv, cv2.COLOR_YUV2BGR)\n",
        "\n",
        "try: \n",
        "  !pip3 install googletrans==3.1.0a0\n",
        "  from googletrans import Translator, constants\n",
        "  from pprint import pprint\n",
        "  translator = Translator()\n",
        "except: pass\n",
        "\n",
        "!pip install bing-image-downloader\n",
        "from bing_image_downloader import downloader\n",
        "\n",
        "%cd /content\n",
        "!pip install transformers==4.15.0 timm==0.4.12 fairscale==0.4.4 scipy==1.7.3\n",
        "!git clone https://github.com/salesforce/BLIP\n",
        "%cd /content/BLIP\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "from models.blip import blip_decoder\n",
        "model_blip = blip_decoder(pretrained='https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth', image_size=384, vit='base')\n",
        "model_blip.eval()\n",
        "model_blip = model_blip.to(device)\n",
        "%cd /content\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize((384,384),interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ]) \n",
        "\n",
        "if os.path.isfile(\"/content/DEVC/colorization_checkpoint.zip\"):\n",
        "  print('Done!')\n",
        "  clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color=\"red\" size=\"+3\">←</font><font color=\"black\" size=\"+2\"> Edit /content/DEVC/test.py</font></b>\n",
        "#@markdown <b><font color=\"black\" size=\"+1\"> (RUS) Исправить файл /content/DEVC/test.py</font></b>\n",
        "%%writefile /content/DEVC/test.py\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.transforms as transform_lib\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import lib.TestTransforms as transforms\n",
        "from models.ColorVidNet import ColorVidNet\n",
        "from models.FrameColor import frame_colorization\n",
        "from models.NonlocalNet import VGG19_pytorch, WarpNet\n",
        "from utils.util import (batch_lab2rgb_transpose_mc, folder2vid, mkdir_if_not,\n",
        "                        save_frames, tensor_lab2rgb, uncenter_l)\n",
        "from utils.util_distortion import CenterPad, Normalize, RGB2Lab, ToTensor\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "\n",
        "def colorize_video(opt, input_path, reference_file, output_path, nonlocal_net, colornet, vggnet):\n",
        "    # parameters for wls filter\n",
        "    wls_filter_on = True\n",
        "    lambda_value = 500\n",
        "    sigma_color = 4\n",
        "\n",
        "    # processing folders\n",
        "    mkdir_if_not(output_path)\n",
        "    files = glob.glob(output_path + \"*\")\n",
        "    print(\"processing the folder:\", input_path)\n",
        "    path, dirs, filenames = os.walk(input_path).__next__()\n",
        "    file_count = len(filenames)\n",
        "    filenames.sort(key=lambda f: int(\"\".join(filter(str.isdigit, f) or -1)))\n",
        "\n",
        "    # NOTE: resize frames to 216*384\n",
        "    transform = transforms.Compose(\n",
        "        [CenterPad(opt.image_size), transform_lib.CenterCrop(opt.image_size), RGB2Lab(), ToTensor(), Normalize()]\n",
        "    )\n",
        "\n",
        "    # if frame propagation: use the first frame as reference\n",
        "    # otherwise, use the specified reference image\n",
        "    ref_name = input_path + filenames[0] if opt.frame_propagate else reference_file\n",
        "    print(\"reference name:\", ref_name)\n",
        "    frame_ref = Image.open(ref_name)\n",
        "\n",
        "    total_time = 0\n",
        "    I_last_lab_predict = None\n",
        "\n",
        "    IB_lab_large = transform(frame_ref).unsqueeze(0).cuda()\n",
        "    IB_lab = torch.nn.functional.interpolate(IB_lab_large, scale_factor=0.5, mode=\"bilinear\")\n",
        "    IB_l = IB_lab[:, 0:1, :, :]\n",
        "    IB_ab = IB_lab[:, 1:3, :, :]\n",
        "    with torch.no_grad():\n",
        "      I_reference_lab = IB_lab\n",
        "      I_reference_l = I_reference_lab[:, 0:1, :, :]\n",
        "      I_reference_ab = I_reference_lab[:, 1:3, :, :]\n",
        "      I_reference_rgb = tensor_lab2rgb(torch.cat((uncenter_l(I_reference_l), I_reference_ab), dim=1))\n",
        "      features_B = vggnet(I_reference_rgb, [\"r12\", \"r22\", \"r32\", \"r42\", \"r52\"], preprocess=True)\n",
        "\n",
        "    for index, frame_name in enumerate(tqdm(filenames)):\n",
        "        frame1 = Image.open(os.path.join(input_path, frame_name))\n",
        "        IA_lab_large = transform(frame1).unsqueeze(0).cuda()\n",
        "        IA_lab = torch.nn.functional.interpolate(IA_lab_large, scale_factor=0.5, mode=\"bilinear\")\n",
        "\n",
        "        IA_l = IA_lab[:, 0:1, :, :]\n",
        "        IA_ab = IA_lab[:, 1:3, :, :]\n",
        "        \n",
        "        if I_last_lab_predict is None:\n",
        "            if opt.frame_propagate:\n",
        "                I_last_lab_predict = IB_lab\n",
        "            else:\n",
        "                I_last_lab_predict = torch.zeros_like(IA_lab).cuda()\n",
        "\n",
        "        # start the frame colorization\n",
        "        with torch.no_grad():\n",
        "            I_current_lab = IA_lab\n",
        "            I_current_ab_predict, I_current_nonlocal_lab_predict, features_current_gray = frame_colorization(\n",
        "                I_current_lab,\n",
        "                I_reference_lab,\n",
        "                I_last_lab_predict,\n",
        "                features_B,\n",
        "                vggnet,\n",
        "                nonlocal_net,\n",
        "                colornet,\n",
        "                feature_noise=0,\n",
        "                temperature=1e-10,\n",
        "            )\n",
        "            I_last_lab_predict = torch.cat((IA_l, I_current_ab_predict), dim=1)\n",
        "\n",
        "        # upsampling\n",
        "        curr_bs_l = IA_lab_large[:, 0:1, :, :]\n",
        "        curr_predict = (\n",
        "            torch.nn.functional.interpolate(I_current_ab_predict.data.cpu(), scale_factor=2, mode=\"bilinear\") * 1.25\n",
        "        )\n",
        "\n",
        "        # filtering\n",
        "        if wls_filter_on:\n",
        "            guide_image = uncenter_l(curr_bs_l) * 255 / 100\n",
        "            wls_filter = cv2.ximgproc.createFastGlobalSmootherFilter(\n",
        "                guide_image[0, 0, :, :].cpu().numpy().astype(np.uint8), lambda_value, sigma_color\n",
        "            )\n",
        "            curr_predict_a = wls_filter.filter(curr_predict[0, 0, :, :].cpu().numpy())\n",
        "            curr_predict_b = wls_filter.filter(curr_predict[0, 1, :, :].cpu().numpy())\n",
        "            curr_predict_a = torch.from_numpy(curr_predict_a).unsqueeze(0).unsqueeze(0)\n",
        "            curr_predict_b = torch.from_numpy(curr_predict_b).unsqueeze(0).unsqueeze(0)\n",
        "            curr_predict_filter = torch.cat((curr_predict_a, curr_predict_b), dim=1)\n",
        "            IA_predict_rgb = batch_lab2rgb_transpose_mc(curr_bs_l[:32], curr_predict_filter[:32, ...])\n",
        "        else:\n",
        "            IA_predict_rgb = batch_lab2rgb_transpose_mc(curr_bs_l[:32], curr_predict[:32, ...])\n",
        "\n",
        "        # save the frames\n",
        "        save_frames(IA_predict_rgb, output_path, index)\n",
        "\n",
        "    # output video\n",
        "    video_name = \"video.avi\"\n",
        "    folder2vid(image_folder=output_path, output_dir=output_path, filename=video_name)\n",
        "    print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--frame_propagate\", default=False, type=bool, help=\"propagation mode, , please check the paper\"\n",
        "    )\n",
        "    parser.add_argument(\"--image_size\", type=int, default=[216 * 2, 384 * 2], help=\"the image size, eg. [216,384]\")\n",
        "    parser.add_argument(\"--cuda\", action=\"store_false\")\n",
        "    parser.add_argument(\"--gpu_ids\", type=str, default=\"0\", help=\"separate by comma\")\n",
        "    parser.add_argument(\"--clip_path\", type=str, default=\"./sample_videos/clips/v32\", help=\"path of input clips\")\n",
        "    parser.add_argument(\"--ref_path\", type=str, default=\"./sample_videos/ref/v32\", help=\"path of refernce images\")\n",
        "    parser.add_argument(\"--output_path\", type=str, default=\"./sample_videos/output\", help=\"path of output clips\")\n",
        "    opt = parser.parse_args()\n",
        "    opt.gpu_ids = [int(x) for x in opt.gpu_ids.split(\",\")]\n",
        "    cudnn.benchmark = True\n",
        "    print(\"running on GPU\", opt.gpu_ids)\n",
        "\n",
        "    clip_name = opt.clip_path.split(\"/\")[-1]\n",
        "    refs = os.listdir(opt.ref_path)\n",
        "    refs.sort()\n",
        "\n",
        "    nonlocal_net = WarpNet(1)\n",
        "    colornet = ColorVidNet(7)\n",
        "    vggnet = VGG19_pytorch()\n",
        "    vggnet.load_state_dict(torch.load(\"data/vgg19_conv.pth\"))\n",
        "    for param in vggnet.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    nonlocal_test_path = os.path.join(\"checkpoints/\", \"video_moredata_l1/nonlocal_net_iter_76000.pth\")\n",
        "    color_test_path = os.path.join(\"checkpoints/\", \"video_moredata_l1/colornet_iter_76000.pth\")\n",
        "    print(\"succesfully load nonlocal model: \", nonlocal_test_path)\n",
        "    print(\"succesfully load color model: \", color_test_path)\n",
        "    nonlocal_net.load_state_dict(torch.load(nonlocal_test_path))\n",
        "    colornet.load_state_dict(torch.load(color_test_path))\n",
        "\n",
        "    nonlocal_net.eval()\n",
        "    colornet.eval()\n",
        "    vggnet.eval()\n",
        "    nonlocal_net.cuda()\n",
        "    colornet.cuda()\n",
        "    vggnet.cuda()\n",
        "\n",
        "    for ref_name in refs:\n",
        "        try:\n",
        "            colorize_video(\n",
        "                opt,\n",
        "                opt.clip_path,\n",
        "                os.path.join(opt.ref_path, ref_name),\n",
        "                os.path.join(opt.output_path, clip_name + \"_\" + ref_name.split(\".\")[0]),\n",
        "                nonlocal_net,\n",
        "                colornet,\n",
        "                vggnet,\n",
        "            )\n",
        "        except Exception as error:\n",
        "            print(\"error when colorizing the video \" + ref_name)\n",
        "            print(error)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "K9DJA6mgJxvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color=\"red\" size=\"+3\">←</font><font color=\"black\" size=\"+3\"> Prepare video</font></b>\n",
        "#@markdown <b><font color=\"black\" size=\"+1\"> (RUS) Подготовка видео</font></b>\n",
        "path_to_video = '' #@param {type:\"string\"}\n",
        "\n",
        "!rm -rf /content/i.mp4 /content/audio.aac\n",
        "try:\n",
        "  !ffmpeg -y -i \"$path_to_video\" -vn -acodec copy /content/audio.aac -hide_banner -loglevel error\n",
        "except: pass\n",
        "fps_of_video = round(cv2.VideoCapture(path_to_video).get(cv2.CAP_PROP_FPS))\n",
        "!ffmpeg -i \"$path_to_video\" -vf format=gray /content/i.mp4 -hide_banner -loglevel error\n",
        "%cd /content\n",
        "!rm -rf /content/scenes $path_to_video\n",
        "!mkdir /content/scenes\n",
        "%cd /content/scenes\n",
        "if os.path.isfile(\"/content/i.mp4\"):\n",
        "  print('\\t Start splitting video into scenes!')\n",
        "  clear_output()\n",
        "!scenedetect -i /content/i.mp4 split-video detect-content -t 10\n",
        "if len(os.listdir('/content/scenes')) >= 1:\n",
        "  print('Done!')\n",
        "  clear_output()\n",
        "#!scenedetect -i /content/i.mp4 --stats /content/stats.csv detect-content"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bJhLs6JUyyrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color=\"red\" size=\"+3\">←</font><font color=\"black\" size=\"+3\"> Colorize</font></b>\n",
        "#@markdown <b><font color=\"black\" size=\"+1\"> (RUS) Колоризовать</font></b>\n",
        "control = False #@param {type:\"boolean\"}\n",
        "!rm -rf /content/DEVC/sample_videos/* /content/temp/*\n",
        "!mkdir /content/DEVC/sample_videos/result /content/DEVC/sample_videos/output\n",
        "scenes_name = os.listdir('/content/scenes')\n",
        "scenes_name.sort()\n",
        "%cd /content/DEVC\n",
        "count = 1\n",
        "for i in tqdm(scenes_name):\n",
        "  only_name = i.split('.')[0]\n",
        "  !mkdir -p \"/content/DEVC/sample_videos/clips/$only_name\"\n",
        "  !mkdir -p \"/content/DEVC/sample_videos/ref/$only_name\"\n",
        "  !ffmpeg -y -i \"/content/scenes/$i\" \"/content/DEVC/sample_videos/clips/$only_name/%7d.png\" -hide_banner -loglevel error\n",
        "\n",
        "  raw_image = Image.open(f'/content/DEVC/sample_videos/clips/{only_name}/0000001.png').convert('RGB')\n",
        "  image = transform(raw_image).unsqueeze(0).to(device)\n",
        "  with torch.no_grad():\n",
        "    caption = model_blip.generate(image, sample=False, num_beams=3, max_length=20, min_length=5) \n",
        "    true_caption = caption[0].replace('in black and white', '')\n",
        "    true_caption += ' color photo'\n",
        "  if control:\n",
        "    text = input(f'\\n /content/DEVC/sample_videos/clips/{only_name}/0000001.png is \"{true_caption}\":')\n",
        "    if text != '':\n",
        "      try:\n",
        "        translator = Translator()\n",
        "        translation = translator.translate(text)\n",
        "        true_caption = translation.text\n",
        "      except: true_caption = text \n",
        "  downloaded_img_path = '/content/some'\n",
        "  while true_caption != '':\n",
        "    !rm -rf /content/temp/* $downloaded_img_path\n",
        "    downloader.download(true_caption, limit=1,  output_dir=f'/content/temp', adult_filter_off=True, force_replace=False, timeout=60, verbose=False)\n",
        "    downloaded_img_path = glob.glob(f'/content/temp/{true_caption}/*.*')[0]\n",
        "    if control:\n",
        "      text = input(f'\\n {downloaded_img_path} - is normal image? ')\n",
        "      try:\n",
        "        translator = Translator()\n",
        "        translation = translator.translate(text)\n",
        "        true_caption = translation.text\n",
        "      except: true_caption = text \n",
        "    else: true_caption = ''\n",
        "  !cp \"$downloaded_img_path\" \"/content/DEVC/sample_videos/ref/$only_name/\"\n",
        "\n",
        "  !python test.py --clip_path \"./sample_videos/clips/$only_name\" \\\n",
        "               --ref_path \"./sample_videos/ref/$only_name\" \\\n",
        "               --output_path \"./sample_videos/output\" \\\n",
        "               --cuda --gpu_ids 0\n",
        "  for_del = f'/content/DEVC/sample_videos/output/{only_name}_Image_1/video.avi'\n",
        "  !rm -rf $for_del\n",
        "  \n",
        "  frame_list = glob.glob(f'/content/DEVC/sample_videos/output/{only_name}_Image_1/*.*')\n",
        "  frame_list.sort()\n",
        "  for frame in tqdm(frame_list):\n",
        "    color = cv2.imread(frame)\n",
        "    frame_name = f'/content/DEVC/sample_videos/clips/{only_name}/' + str(int(os.path.basename(frame).split('.')[0])+1).zfill(7) + '.png'\n",
        "    orig = cv2.imread(frame_name)\n",
        "    color = cv2.resize(color, (orig.shape[1], orig.shape[0]))\n",
        "    result = color_restore(orig, color)\n",
        "    result_name = str(len(glob.glob('/content/DEVC/sample_videos/result/*.*'))).zfill(7)\n",
        "    cv2.imwrite(f'/content/DEVC/sample_videos/result/{result_name}.jpg', result)\n",
        "\n",
        "  if len(os.listdir('/content/DEVC/sample_videos/output')) == count:\n",
        "    count += 1\n",
        "    clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "c1T6JCDk1QMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color=\"red\" size=\"+3\">←</font><font color=\"black\" size=\"+3\"> Get result</font></b>\n",
        "#@markdown <b><font color=\"black\" size=\"+1\"> (RUS) Получить результат</font></b>\n",
        "if os.path.isfile(\"/content/audio.aac\"):\n",
        "  !ffmpeg -y -r $fps_of_video -i /content/DEVC/sample_videos/result/%07d.jpg -c:v libx264 -vf fps=$fps_of_video -pix_fmt yuv420p /content/temp_result.mp4 -hide_banner -loglevel error\n",
        "  !ffmpeg -y -i /content/temp_result.mp4 -i /content/audio.aac /content/result.mp4 -hide_banner -loglevel error\n",
        "  !rm -rf /content/temp_result.mp4\n",
        "else: \n",
        "  !ffmpeg -y -r $fps_of_video -i /content/DEVC/sample_videos/result/%07d.jpg -c:v libx264 -vf fps=$fps_of_video -pix_fmt yuv420p /content/result.mp4 -hide_banner -loglevel error\n",
        "\n",
        "print('Result here: /content/result.mp4')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "edBObYtKvfEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}